{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "461a9f27-6179-4551-8fa2-39eb1d10ec75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import json\n",
    "import os\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dcd2b9c-2f81-4ba9-a049-636b873577a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b6e1f1-0c8b-4159-b8b5-bda1b8e6868e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Total samples: 82,603\n",
      "Train : 57,564\n",
      "Test  : 25,039\n",
      "Assigned Weights (Normalized Rank):\n",
      "  mistral: 0.067 (Acc: 51.86%)\n",
      "  chatglm: 0.133 (Acc: 75.67%)\n",
      "  llama: 0.200 (Acc: 79.29%)\n",
      "  qwen: 0.267 (Acc: 87.16%)\n",
      "  gemma: 0.333 (Acc: 90.27%)\n",
      "\n",
      "====================================================================================================\n",
      "                                  LLM JUDGE EVALUATION (TEST SET)                                   \n",
      "====================================================================================================\n",
      "           Method Accuracy Hallu Risk Precision    F1\n",
      "         BGE@0.85    51.3%      41.4%     0.430 0.423\n",
      "     BGE@0.85 Key    42.2%      99.7%     0.424 0.592\n",
      "  BGE@0.85 Intent    42.2%      99.7%     0.424 0.592\n",
      "             Qwen    65.9%       6.9%     0.764 0.426\n",
      "         Qwen Aug    63.1%       2.4%     0.842 0.285\n",
      "            Llama    61.5%      18.7%     0.586 0.440\n",
      "        Llama Aug    63.6%      27.6%     0.585 0.550\n",
      "          Mistral    42.9%      99.7%     0.428 0.599\n",
      "      Mistral Aug    43.3%      97.2%     0.429 0.596\n",
      "            Gemma    58.6%      42.0%     0.515 0.552\n",
      "        Gemma Aug    60.9%      38.4%     0.539 0.567\n",
      "          ChatGLM    48.9%      85.8%     0.454 0.615\n",
      "      ChatGLM Aug    51.5%      80.0%     0.468 0.624\n",
      "             GPT5    55.7%      61.3%     0.489 0.602\n",
      "    Majority Vote    60.2%      49.2%     0.526 0.611\n",
      "Majority Vote Aug    61.7%      47.6%     0.538 0.623\n",
      "    Weighted Vote    59.6%      41.1%     0.525 0.563\n",
      "     neural_model    64.2%      32.5%     0.580 0.588\n",
      "            ral2m    70.7%      13.9%     0.730 0.595\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==================== Load data safely ====================\n",
    "print(\"Loading data...\")\n",
    "# Update this path if your file is named differently\n",
    "file_path = \"raw_results/complete_judge_result.jsonl\" \n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    # Dummy data creation for code integrity if file doesn't exist\n",
    "    print(f\"File {file_path} not found. Ensure your data is in the current directory.\")\n",
    "    exit()\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line.strip()) for line in f if line.strip()]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(f\"Total samples: {len(df):,}\")\n",
    "\n",
    "# Split identification\n",
    "train_df = df[df['split'] == 'train'].reset_index(drop=True)\n",
    "test_df  = df[df['split'] == 'test'].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train : {len(train_df):,}\")\n",
    "print(f\"Test  : {len(test_df):,}\")\n",
    "\n",
    "def evaluate_judge(df_subset, method):\n",
    "    if len(df_subset) == 0:\n",
    "        return None\n",
    "\n",
    "    # --- Logic for extracting \"Accepted\" Mask ---\n",
    "    if method == \"BGE@0.85\":\n",
    "        accepted = df_subset['bge_retrieved_top1_score'] >= 0.85\n",
    "    elif method == \"BGE@0.85 Key\":\n",
    "        accepted = df_subset['bge_keyword_retrieved_top1_score'] >= 0.85\n",
    "    elif method == \"BGE@0.85 Intent\":\n",
    "        accepted = df_subset['bge_intent_retrieved_top1_score'] >= 0.85\n",
    "    # Cleaned Base Names\n",
    "    elif method == \"Qwen\":\n",
    "        accepted = df_subset['qwen'].fillna(False).astype(bool)\n",
    "    elif method == \"Llama\":\n",
    "        accepted = df_subset['llama'].fillna(False).astype(bool)\n",
    "    elif method == \"Mistral\":\n",
    "        accepted = df_subset['mistral'].fillna(False).astype(bool)\n",
    "    elif method == \"Gemma\":\n",
    "        accepted = df_subset['gemma'].fillna(False).astype(bool)\n",
    "    elif method == \"ChatGLM\":\n",
    "        accepted = df_subset['chatglm'].fillna(False).astype(bool)\n",
    "    \n",
    "    # Augmented Methods (using the y_true fallback versions)\n",
    "    elif method == \"Qwen Aug\":\n",
    "        accepted = df_subset['qwen_aug'].astype(bool)\n",
    "    elif method == \"Llama Aug\":\n",
    "        accepted = df_subset['llama_aug'].astype(bool)\n",
    "    elif method == \"Mistral Aug\":\n",
    "        accepted = df_subset['mistral_aug'].astype(bool)\n",
    "    elif method == \"Gemma Aug\":\n",
    "        accepted = df_subset['gemma_aug'].astype(bool)\n",
    "\n",
    "    elif method == \"ChatGLM Aug\":\n",
    "        accepted = df_subset['chatglm_aug'].astype(bool)\n",
    "\n",
    "    elif method == \"GPT5\":\n",
    "        accepted = df_subset['gpt5'].astype(bool)\n",
    "    \n",
    "    elif method == \"Majority Vote\":\n",
    "        # Using the cleaned names\n",
    "        base_judges = ['qwen', 'llama', 'mistral', 'gemma', 'chatglm']\n",
    "        accepted = df_subset[base_judges].fillna(0).sum(axis=1) >= 3\n",
    "        vote_df = df_subset[base_judges].fillna(0).astype(int)\n",
    "        true_votes = vote_df.sum(axis=1)\n",
    "        false_votes = 5 - true_votes\n",
    "        accepted = true_votes > false_votes\n",
    "    elif method == \"Majority Vote Aug\":\n",
    "            # Using the cleaned names\n",
    "            base_judges = ['qwen_aug', 'llama_aug', 'mistral_aug', 'gemma_aug', 'chatglm_aug']\n",
    "            accepted = df_subset[base_judges].fillna(0).sum(axis=1) >= 3\n",
    "            vote_df = df_subset[base_judges].fillna(0).astype(int)\n",
    "            true_votes = vote_df.sum(axis=1)\n",
    "            false_votes = 5 - true_votes\n",
    "            accepted = true_votes > false_votes\n",
    "    elif method == \"Weighted Vote\":\n",
    "        base_judges = ['qwen', 'llama', 'mistral', 'gemma', 'chatglm']\n",
    "        # We calculate the weighted sum of bipolar votes\n",
    "        weighted_score = pd.Series(0.0, index=df_subset.index)\n",
    "        \n",
    "        for judge in base_judges:\n",
    "            # Map True -> 1, False -> -1\n",
    "            # .astype(bool) ensures logic holds for different data types\n",
    "            v_i = df_subset[judge].fillna(False).astype(bool).map({True: 1, False: -1})\n",
    "            weighted_score += v_i * learned_weights[judge]\n",
    "            \n",
    "        # If the weighted sum > 0, the collective decision is True\n",
    "        accepted = weighted_score > 0\n",
    "    else:\n",
    "        accepted = df_subset[method.lower()]\n",
    "    if method != \"BGE@0.85 Key\":\n",
    "        y_true = df_subset['y_true'].astype(bool)\n",
    "    else:\n",
    "        y_true = (df_subset['sample_class'] == 'positive') & (\n",
    "        df_subset['bge_keyword_retrieved_top1_id'] == df_subset['question_id']\n",
    "    )\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, accepted, labels=[False, True]).ravel()\n",
    "    \n",
    "    total = len(y_true)\n",
    "    n_pos = tp + fn\n",
    "    n_neg = tn + fp\n",
    "    \n",
    "    accuracy = (tp + tn) / total\n",
    "    recall = tp / n_pos if n_pos > 0 else 0.0\n",
    "    specificity = tn / n_neg if n_neg > 0 else 0.0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    hallu_risk = fp / (fp + tn)\n",
    "    accept_ratio = accepted.sum() / y_true.sum() if y_true.sum() > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"Method\": method,\n",
    "        \"Accept%\": accept_ratio,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Hallu Risk\": hallu_risk,\n",
    "        \"Recall%\": recall,\n",
    "        \"Neg Reject%\": specificity,\n",
    "        \"Precision\": precision,\n",
    "        \"F1\": f1\n",
    "    }\n",
    "\n",
    "# --- Execution ---\n",
    "methods = [\n",
    "    \"BGE@0.85\", \n",
    "    \"BGE@0.85 Key\", \n",
    "    \"BGE@0.85 Intent\",\n",
    "    \"Qwen\", \"Qwen Aug\",\n",
    "    \"Llama\", \"Llama Aug\",\n",
    "    \"Mistral\", \"Mistral Aug\",\n",
    "    \"Gemma\", \"Gemma Aug\",\n",
    "    \"ChatGLM\", \"ChatGLM Aug\",\n",
    "    \"GPT5\",\n",
    "    \"Majority Vote\", \"Majority Vote Aug\",\n",
    "    \"Weighted Vote\",\n",
    "    \"neural_model\",\n",
    "    \"ral2m\"\n",
    "]\n",
    "\n",
    "# 1. PRE-CALCULATE WEIGHTS (Run this before defining/calling the eval loop)\n",
    "base_judges = ['qwen', 'llama', 'mistral', 'gemma', 'chatglm']\n",
    "\n",
    "# Calculate accuracy on train split to determine ranks\n",
    "judge_accs = {}\n",
    "for judge in base_judges:\n",
    "    correct = (train_df[judge].fillna(False).astype(bool) == train_df['y_true'].astype(bool))\n",
    "    judge_accs[judge] = correct.mean()\n",
    "\n",
    "# Sort judges by accuracy (ascending)\n",
    "sorted_judges = sorted(base_judges, key=lambda x: judge_accs[x])\n",
    "\n",
    "# Assign normalized rank weights: rank / sum(ranks)\n",
    "# For 5 judges, sum is 15\n",
    "sum_ranks = sum(range(1, len(base_judges) + 1))\n",
    "learned_weights = {judge: (i + 1) / sum_ranks for i, judge in enumerate(sorted_judges)}\n",
    "\n",
    "print(\"Assigned Weights (Normalized Rank):\")\n",
    "for j in sorted_judges:\n",
    "    print(f\"  {j}: {learned_weights[j]:.3f} (Acc: {judge_accs[j]:.2%})\")\n",
    "\n",
    "results = [evaluate_judge(test_df, m) for m in methods if evaluate_judge(test_df, m)]\n",
    "\n",
    "final_df = pd.DataFrame(results)\n",
    "# final_df = final_df[[\"Method\", \"Accept%\", \"Accuracy\", \"Hallu Risk\", \"Recall%\", \"Neg Reject%\", \"Precision\", \"F1\"]]\n",
    "final_df = final_df[[\"Method\", \"Accuracy\", \"Hallu Risk\", \"Precision\", \"F1\"]]\n",
    "\n",
    "# Formatting\n",
    "display_df = final_df.copy()\n",
    "# display_df[\"Accept%\"] = display_df[\"Accept%\"].map(\"{:.1%}\".format)\n",
    "display_df[\"Accuracy\"] = display_df[\"Accuracy\"].map(\"{:.1%}\".format)\n",
    "display_df[\"Hallu Risk\"] = display_df[\"Hallu Risk\"].map(\"{:.1%}\".format)\n",
    "# display_df[\"Recall%\"] = display_df[\"Recall%\"].map(\"{:.1%}\".format)\n",
    "# display_df[\"Neg Reject%\"] = display_df[\"Neg Reject%\"].map(\"{:.1%}\".format)\n",
    "display_df[\"Precision\"] = display_df[\"Precision\"].map(\"{:.3f}\".format)\n",
    "display_df[\"F1\"] = display_df[\"F1\"].map(\"{:.3f}\".format)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(f\"{'LLM JUDGE EVALUATION (TEST SET)':^100}\")\n",
    "print(\"=\"*100)\n",
    "print(display_df.to_string(index=False))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3433476b-aafc-491e-8ea8-699c26fbc138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "                                  LLM JUDGE EVALUATION (TEST SET)                                   \n",
      "                      Stat. Significance (ral2m vs majority_vote): p = 0.0000                       \n",
      "====================================================================================================\n",
      "           Method Accuracy Hallu Risk Precision    F1\n",
      "         BGE@0.85    51.3%      41.4%     0.430 0.423\n",
      "     BGE@0.85 Key    42.2%      99.7%     0.424 0.592\n",
      "  BGE@0.85 Intent    42.2%      99.7%     0.424 0.592\n",
      "             Qwen    65.9%       6.9%     0.764 0.426\n",
      "         Qwen Aug    63.1%       2.4%     0.842 0.285\n",
      "            Llama    61.5%      18.7%     0.586 0.440\n",
      "        Llama Aug    63.6%      27.6%     0.585 0.550\n",
      "          Mistral    42.9%      99.7%     0.428 0.599\n",
      "      Mistral Aug    43.3%      97.2%     0.429 0.596\n",
      "            Gemma    58.6%      42.0%     0.515 0.552\n",
      "        Gemma Aug    60.9%      38.4%     0.539 0.567\n",
      "          ChatGLM    48.9%      85.8%     0.454 0.615\n",
      "      ChatGLM Aug    51.5%      80.0%     0.468 0.624\n",
      "             GPT5    55.7%      61.3%     0.489 0.602\n",
      "    Majority Vote    60.2%      49.2%     0.526 0.611\n",
      "Majority Vote Aug    61.7%      47.6%     0.538 0.623\n",
      "    Weighted Vote    59.6%      41.1%     0.525 0.563\n",
      "     neural_model    64.2%      32.5%     0.580 0.588\n",
      "            ral2m 70.7%***      13.9%     0.730 0.595\n",
      "====================================================================================================\n",
      "Significance levels: * p < 0.05, ** p < 0.01, *** p < 0.001\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import binom\n",
    "\n",
    "def get_mcnemar_p_value(df, model_a_name, model_b_name):\n",
    "    \"\"\"\n",
    "    Computes the exact p-value using the binomial distribution.\n",
    "    model_a_name: Name of the baseline in the dataframe (e.g., 'majority_vote')\n",
    "    model_b_name: Name of the target model in the dataframe (e.g., 'ral2m')\n",
    "    \"\"\"\n",
    "    y_true = df['y_true'].astype(bool)\n",
    "    \n",
    "    # Identify correct predictions for both models\n",
    "    # Note: ensure column names match your dataframe keys exactly\n",
    "    correct_a = (df[model_a_name].fillna(False).astype(bool) == y_true)\n",
    "    correct_b = (df[model_b_name].fillna(False).astype(bool) == y_true)\n",
    "    \n",
    "    # Count discordant pairs\n",
    "    # b: Baseline correct, but ral2m wrong\n",
    "    # c: ral2m correct, but Baseline wrong\n",
    "    b = (correct_a & ~correct_b).sum()\n",
    "    c = (~correct_a & correct_b).sum()\n",
    "    \n",
    "    # n is the total number of disagreements\n",
    "    n = b + c\n",
    "    if n == 0:\n",
    "        return 1.0\n",
    "    \n",
    "    # The exact p-value is based on the binomial distribution B(n, 0.5)\n",
    "    # We calculate the probability of seeing a split as extreme as b and c\n",
    "    k = min(b, c)\n",
    "    p_value = 2 * binom.cdf(k, n, 0.5)\n",
    "    return min(1.0, p_value)\n",
    "\n",
    "# --- Statistical Calculation Logic ---\n",
    "\n",
    "# We compare against 'Majority Vote' as the primary baseline\n",
    "# Note: Ensure these strings match the actual columns in your test_df\n",
    "# If 'Majority Vote' is computed inside evaluate_judge, you may need to \n",
    "# pre-compute it in test_df before running this test.\n",
    "baseline_key = 'majority_vote' \n",
    "target_key = 'ral2m'\n",
    "\n",
    "# Check if keys exist in df, if not, create them for the test\n",
    "if baseline_key not in test_df.columns:\n",
    "    base_judges = ['qwen', 'llama', 'mistral', 'gemma', 'chatglm']\n",
    "    test_df[baseline_key] = test_df[base_judges].fillna(0).sum(axis=1) >= 3\n",
    "\n",
    "p_val = get_mcnemar_p_value(test_df, baseline_key, target_key)\n",
    "\n",
    "# Determine Significance Asterisks\n",
    "sig_stars = \"\"\n",
    "if p_val < 0.001: sig_stars = \"***\"\n",
    "elif p_val < 0.01: sig_stars = \"**\"\n",
    "elif p_val < 0.05: sig_stars = \"*\"\n",
    "\n",
    "# Apply formatting to the display_df\n",
    "display_df['Accuracy'] = display_df.apply(\n",
    "    lambda x: f\"{x['Accuracy']}{sig_stars}\" if x['Method'] == 'ral2m' else x['Accuracy'], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# --- Final Output ---\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(f\"{'LLM JUDGE EVALUATION (TEST SET)':^100}\")\n",
    "print(f\"{f'Stat. Significance (ral2m vs {baseline_key}): p = {p_val:.4f}':^100}\")\n",
    "print(\"=\"*100)\n",
    "print(display_df.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "print(f\"Significance levels: * p < 0.05, ** p < 0.01, *** p < 0.001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "029ce1a2-d328-48c1-8d63-f881f854f797",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['qwen', 'llama', 'mistral', 'gemma', 'chatglm']\n",
    "\n",
    "# 1. Prepare base data\n",
    "y_true = test_df['y_true'].astype(int)\n",
    "vote_df = test_df[cols].fillna(0).astype(int)\n",
    "\n",
    "# 2. Calculate counts per sample\n",
    "num_yes = vote_df.sum(axis=1)\n",
    "num_no = 5 - num_yes\n",
    "\n",
    "# 3. Calculate \"Correctness\" (Alignment to $y_{true}$)\n",
    "# This compares each judge column to the ground truth\n",
    "alignment_df = vote_df.apply(lambda col: col == y_true)\n",
    "num_correct = alignment_df.sum(axis=1)\n",
    "\n",
    "# 4. Create the new tracking DataFrame\n",
    "# This records the breakdown for every single sample\n",
    "sample_stats_df = pd.DataFrame({\n",
    "    'num_yes': num_yes,\n",
    "    'num_no': num_no,\n",
    "    'num_correct': num_correct,\n",
    "    'ground_truth': y_true\n",
    "})\n",
    "\n",
    "# 5. Determine majority decision and binary correctness\n",
    "# The judge majority votes '1' if num_yes >= 3\n",
    "majority_decision = (num_yes >= 3).astype(int)\n",
    "# The majority is correct ONLY if the decision matches ground truth\n",
    "is_majority_correct = (majority_decision == y_true)\n",
    "\n",
    "sample_stats_df['majority_decision'] = majority_decision\n",
    "sample_stats_df['is_majority_correct'] = is_majority_correct\n",
    "\n",
    "# The final 'accepted' Series for your evaluation function\n",
    "accepted = majority_decision.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d48c61d1-3e80-426b-bff2-f120383570de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Correctness Data (Main Plot) ---\n",
      "$k = 5$: 5.13%\n",
      "$k \\geq 4$: 23.32%\n",
      "$k \\geq 3$: 60.23%\n",
      "$k \\geq 2$: 90.55%\n",
      "$k \\geq 1$: 98.48%\n",
      "\n",
      "--- Consensus Data (Inset Plot) ---\n",
      "          Level  Percentage\n",
      "0    Full (5/5)    6.649627\n",
      "1  Strong (4/1)   26.111266\n",
      "2    Weak (3/2)   67.239107\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Data Processing ---\n",
    "# (Assumes test_df is already loaded)\n",
    "\n",
    "cols = ['qwen', 'llama', 'mistral', 'gemma', 'chatglm']\n",
    "\n",
    "# A. Prepare base data\n",
    "y_true = test_df['y_true'].astype(int)\n",
    "vote_df = test_df[cols].fillna(0).astype(int)\n",
    "\n",
    "# B. Explicitly calculate counts and correctness per sample\n",
    "num_yes = vote_df.sum(axis=1)\n",
    "num_no = 5 - num_yes\n",
    "\n",
    "# Alignment: Compare each judge column strictly to the ground truth\n",
    "alignment_df = vote_df.apply(lambda col: col == y_true)\n",
    "num_correct = alignment_df.sum(axis=1)\n",
    "\n",
    "# C. Build the Stats DataFrame\n",
    "sample_stats_df = pd.DataFrame({\n",
    "    'num_yes': num_yes,\n",
    "    'num_no': num_no,\n",
    "    'num_correct': num_correct,\n",
    "    'ground_truth': y_true\n",
    "})\n",
    "\n",
    "# D. Define Majority Decision Logic\n",
    "majority_decision = (num_yes >= 3).astype(int)\n",
    "is_majority_correct = (majority_decision == y_true)\n",
    "\n",
    "sample_stats_df['majority_decision'] = majority_decision\n",
    "sample_stats_df['is_majority_correct'] = is_majority_correct\n",
    "\n",
    "# --- 2. Generate Correctness Plotting Data (k-values) ---\n",
    "acc_counts = sample_stats_df['num_correct'].value_counts().sort_index()\n",
    "total = len(sample_stats_df)\n",
    "\n",
    "k_values = []\n",
    "percentages = []\n",
    "cumulative_correct = 0\n",
    "\n",
    "# Calculating cumulative percentages for k = 5, 4, 3, 2, 1\n",
    "for k in range(5, 0, -1):\n",
    "    cumulative_correct += acc_counts.get(k, 0)\n",
    "    if k == 5:\n",
    "        k_values.append(f\"$k = {k}$\")\n",
    "    else:\n",
    "        k_values.append(f\"$k \\\\geq {k}$\")\n",
    "    percentages.append((cumulative_correct / total) * 100)\n",
    "\n",
    "# --- 3. Generate Consensus Plotting Data (Agreement) ---\n",
    "def get_consensus_level(row):\n",
    "    \"\"\"Categorizes the agreement level for a 5-judge panel.\"\"\"\n",
    "    top_vote_count = row.value_counts().max()\n",
    "    if top_vote_count == 5:\n",
    "        return \"Full (5/5)\"\n",
    "    elif top_vote_count == 4:\n",
    "        return \"Strong (4/1)\"\n",
    "    elif top_vote_count == 3:\n",
    "        return \"Weak (3/2)\"\n",
    "    else:\n",
    "        return \"Major Discrepancy (Split)\"\n",
    "\n",
    "# Apply function to get the series of levels\n",
    "consensus_series = vote_df.apply(get_consensus_level, axis=1)\n",
    "\n",
    "# Create the consensus_df\n",
    "consensus_counts = consensus_series.value_counts(normalize=True) * 100\n",
    "consensus_df = consensus_counts.reset_index()\n",
    "consensus_df.columns = ['Level', 'Percentage']\n",
    "\n",
    "# Define the logical order and sort\n",
    "level_order = [\"Full (5/5)\", \"Strong (4/1)\", \"Weak (3/2)\"]\n",
    "consensus_df['Level'] = pd.Categorical(consensus_df['Level'], categories=level_order, ordered=True)\n",
    "consensus_df = consensus_df.sort_values('Level').reset_index(drop=True)\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"--- Correctness Data (Main Plot) ---\")\n",
    "for k, p in zip(k_values, percentages):\n",
    "    print(f\"{k}: {p:.2f}%\")\n",
    "\n",
    "print(\"\\n--- Consensus Data (Inset Plot) ---\")\n",
    "print(consensus_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c2a9d46-19cf-49fe-86b2-4812388e6fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples with $k \\geq 3$ judges correct: 60.23%\n",
      "Actual Majority Vote Accuracy: 60.23%\n"
     ]
    }
   ],
   "source": [
    "# DEBUG: Print the relationship to see why accuracy != 80%\n",
    "total_samples = len(sample_stats_df)\n",
    "samples_with_3_correct = (sample_stats_df['num_correct'] >= 3).sum()\n",
    "actual_accuracy = is_majority_correct.sum()\n",
    "\n",
    "print(f\"Samples with $k \\geq 3$ judges correct: {samples_with_3_correct / total_samples:.2%}\")\n",
    "print(f\"Actual Majority Vote Accuracy: {actual_accuracy / total_samples:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b242af96-d6ca-4411-bc20-3dff36b4009a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Global Config ---\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "sns.set_context(\"paper\", font_scale=1.1)\n",
    "\n",
    "# 1. Initialize Figure\n",
    "fig = plt.figure(figsize=(5, 3), dpi=1000)\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "# Reverse k_values to plot as 5,4,3,2,1\n",
    "k_values_rev = k_values[::-1]\n",
    "percentages_rev = percentages[::-1]\n",
    "\n",
    "ax1 = sns.barplot(x=k_values_rev, y=percentages_rev, palette=\"Blues_d\",\n",
    "                  edgecolor=\"0.2\", linewidth=1, width=0.8, ax=ax1)\n",
    "\n",
    "# ax1.set_ylabel(\"Samples by Correct Judges (%)\", fontsize=9, fontweight='bold')\n",
    "ax1.yaxis.labelpad = 2\n",
    "ax1.set_xlabel(\"\", fontsize=0)\n",
    "ax1.grid(False)\n",
    "ax1.set_ylim(0, 105) \n",
    "ax1.set_yticks([20, 40, 60, 80, 100])\n",
    "sns.despine(ax=ax1, left=True)\n",
    "ax1.tick_params(labelsize=9, pad=2)\n",
    "ax1.tick_params(axis='y', labelsize=8, pad=0)\n",
    "ax1.tick_params(axis='y', length=0)\n",
    "# Main Plot Annotations\n",
    "for p in ax1.patches:\n",
    "    ax1.annotate(f'{p.get_height():.1f}%',\n",
    "                  (p.get_x() + p.get_width() / 2., p.get_height() + 3),\n",
    "                  ha='center', va='center', fontsize=8, fontweight='500')\n",
    "\n",
    "\n",
    "\n",
    "plt.savefig(\"figures/correctness_distribution.png\", dpi=1600, bbox_inches='tight')\n",
    "plt.savefig(\"figures/correctness_distribution.pdf\", dpi=1600, bbox_inches='tight', format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ad20a0b-90b7-413e-8a14-ec07177bd706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Top-level k-values Table ===\n",
      "         k  Number of Correct Judges (%)\n",
      "$k \\geq 1$                     98.478374\n",
      "$k \\geq 2$                     90.554735\n",
      "$k \\geq 3$                     60.234035\n",
      "$k \\geq 4$                     23.315628\n",
      "   $k = 5$                      5.128000\n",
      "\n",
      "=== Consensus Levels Table ===\n",
      "Consensus Level  Agreement (%)\n",
      "           Full       6.649627\n",
      "         Strong      26.111266\n",
      "           Weak      67.239107\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Table 1: Top-level k-values ---\n",
    "# Reverse to match plotting order (5,4,3,2,1)\n",
    "table_k = pd.DataFrame({\n",
    "    'k': k_values[::-1],\n",
    "    'Number of Correct Judges (%)': percentages[::-1]\n",
    "})\n",
    "\n",
    "print(\"=== Top-level k-values Table ===\")\n",
    "print(table_k.to_string(index=False))\n",
    "\n",
    "# --- Table 2: Consensus Levels ---\n",
    "# Clean Level names (remove text in parentheses)\n",
    "consensus_df['Consensus Level'] = [lvl.split(' (')[0] for lvl in consensus_df['Level']]\n",
    "\n",
    "table_consensus = consensus_df[['Consensus Level', 'Percentage']].rename(\n",
    "    columns={'Percentage': 'Agreement (%)'}\n",
    ")\n",
    "\n",
    "print(\"\\n=== Consensus Levels Table ===\")\n",
    "print(table_consensus.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "25522cc2-e0ca-4799-b4ff-b995c21cddc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cohen's Kappa Matrix ---\n",
      "          qwen  llama  mistral  gemma  chatglm\n",
      "qwen     1.000  0.478    0.004  0.543    0.213\n",
      "llama    0.478  1.000    0.004  0.419    0.197\n",
      "mistral  0.004  0.004    1.000  0.008    0.020\n",
      "gemma    0.543  0.419    0.008  1.000    0.374\n",
      "chatglm  0.213  0.197    0.020  0.374    1.000\n",
      "\n",
      "--- Pearson Correlation Matrix ---\n",
      "                                  query_id  question_id  dataset  user_query  \\\n",
      "query_id                               NaN          NaN      NaN         NaN   \n",
      "question_id                            NaN        1.000      NaN         NaN   \n",
      "dataset                                NaN          NaN      NaN         NaN   \n",
      "user_query                             NaN          NaN      NaN         NaN   \n",
      "sample_class                           NaN          NaN      NaN         NaN   \n",
      "split                                  NaN          NaN      NaN         NaN   \n",
      "qwen                                   NaN       -0.156      NaN         NaN   \n",
      "mistral                                NaN        0.017      NaN         NaN   \n",
      "gemma                                  NaN       -0.082      NaN         NaN   \n",
      "y_true                                 NaN       -0.105      NaN         NaN   \n",
      "llama                                  NaN       -0.064      NaN         NaN   \n",
      "chatglm                                NaN       -0.099      NaN         NaN   \n",
      "bge_retrieved_top1_id                  NaN        0.952      NaN         NaN   \n",
      "bge_retrieved_top1_score               NaN       -0.000      NaN         NaN   \n",
      "bge_keyword_retrieved_top1_id          NaN        0.952      NaN         NaN   \n",
      "bge_keyword_retrieved_top1_score       NaN       -0.000      NaN         NaN   \n",
      "bge_top1_doc_id                        NaN          NaN      NaN         NaN   \n",
      "bge_top1_doc_score                     NaN          NaN      NaN         NaN   \n",
      "qwen_aug                               NaN       -0.108      NaN         NaN   \n",
      "llama_aug                              NaN       -0.089      NaN         NaN   \n",
      "mistral_aug                            NaN        0.103      NaN         NaN   \n",
      "gemma_aug                              NaN       -0.060      NaN         NaN   \n",
      "neural_model                           NaN       -0.098      NaN         NaN   \n",
      "ral2m                                  NaN       -0.011      NaN         NaN   \n",
      "chatglm_aug                            NaN       -0.078      NaN         NaN   \n",
      "gpt5                                   NaN       -0.062      NaN         NaN   \n",
      "bge_intent_retrieved_top1              NaN        0.952      NaN         NaN   \n",
      "bge_intent_retrieved_top1_score        NaN       -0.000      NaN         NaN   \n",
      "\n",
      "                                  sample_class  split   qwen  mistral  gemma  \\\n",
      "query_id                                   NaN    NaN    NaN      NaN    NaN   \n",
      "question_id                                NaN    NaN -0.156    0.017 -0.082   \n",
      "dataset                                    NaN    NaN    NaN      NaN    NaN   \n",
      "user_query                                 NaN    NaN    NaN      NaN    NaN   \n",
      "sample_class                               NaN    NaN    NaN      NaN    NaN   \n",
      "split                                      NaN    NaN    NaN      NaN    NaN   \n",
      "qwen                                       NaN    NaN  1.000    0.040  0.581   \n",
      "mistral                                    NaN    NaN  0.040    1.000  0.064   \n",
      "gemma                                      NaN    NaN  0.581    0.064  1.000   \n",
      "y_true                                     NaN    NaN  0.648    0.037  0.614   \n",
      "llama                                      NaN    NaN  0.478    0.044  0.451   \n",
      "chatglm                                    NaN    NaN  0.342    0.080  0.462   \n",
      "bge_retrieved_top1_id                      NaN    NaN -0.141    0.022 -0.060   \n",
      "bge_retrieved_top1_score                   NaN    NaN  0.005    0.000  0.003   \n",
      "bge_keyword_retrieved_top1_id              NaN    NaN -0.141    0.022 -0.060   \n",
      "bge_keyword_retrieved_top1_score           NaN    NaN  0.005    0.000  0.003   \n",
      "bge_top1_doc_id                            NaN    NaN    NaN      NaN    NaN   \n",
      "bge_top1_doc_score                         NaN    NaN    NaN      NaN    NaN   \n",
      "qwen_aug                                   NaN    NaN  0.680    0.030  0.449   \n",
      "llama_aug                                  NaN    NaN  0.439    0.053  0.455   \n",
      "mistral_aug                                NaN    NaN  0.045    0.150  0.095   \n",
      "gemma_aug                                  NaN    NaN  0.556    0.062  0.847   \n",
      "neural_model                               NaN    NaN  0.693    0.026  0.575   \n",
      "ral2m                                      NaN    NaN  0.325   -0.003  0.132   \n",
      "chatglm_aug                                NaN    NaN  0.356    0.085  0.477   \n",
      "gpt5                                       NaN    NaN -0.105    0.017  0.135   \n",
      "bge_intent_retrieved_top1                  NaN    NaN -0.141    0.022 -0.060   \n",
      "bge_intent_retrieved_top1_score            NaN    NaN  0.005    0.000  0.003   \n",
      "\n",
      "                                  y_true  ...  qwen_aug  llama_aug  \\\n",
      "query_id                             NaN  ...       NaN        NaN   \n",
      "question_id                       -0.105  ...    -0.108     -0.089   \n",
      "dataset                              NaN  ...       NaN        NaN   \n",
      "user_query                           NaN  ...       NaN        NaN   \n",
      "sample_class                         NaN  ...       NaN        NaN   \n",
      "split                                NaN  ...       NaN        NaN   \n",
      "qwen                               0.648  ...     0.680      0.439   \n",
      "mistral                            0.037  ...     0.030      0.053   \n",
      "gemma                              0.614  ...     0.449      0.455   \n",
      "y_true                             1.000  ...     0.493      0.488   \n",
      "llama                              0.505  ...     0.375      0.632   \n",
      "chatglm                            0.452  ...     0.252      0.354   \n",
      "bge_retrieved_top1_id             -0.079  ...    -0.096     -0.073   \n",
      "bge_retrieved_top1_score          -0.003  ...     0.007      0.004   \n",
      "bge_keyword_retrieved_top1_id     -0.079  ...    -0.096     -0.073   \n",
      "bge_keyword_retrieved_top1_score  -0.003  ...     0.007      0.004   \n",
      "bge_top1_doc_id                      NaN  ...       NaN        NaN   \n",
      "bge_top1_doc_score                   NaN  ...       NaN        NaN   \n",
      "qwen_aug                           0.493  ...     1.000      0.415   \n",
      "llama_aug                          0.488  ...     0.415      1.000   \n",
      "mistral_aug                        0.041  ...     0.080      0.109   \n",
      "gemma_aug                          0.591  ...     0.461      0.479   \n",
      "neural_model                       0.650  ...     0.488      0.415   \n",
      "ral2m                              0.194  ...     0.236      0.137   \n",
      "chatglm_aug                        0.464  ...     0.267      0.382   \n",
      "gpt5                               0.001  ...    -0.105      0.038   \n",
      "bge_intent_retrieved_top1         -0.079  ...    -0.096     -0.073   \n",
      "bge_intent_retrieved_top1_score   -0.003  ...     0.007      0.004   \n",
      "\n",
      "                                  mistral_aug  gemma_aug  neural_model  ral2m  \\\n",
      "query_id                                  NaN        NaN           NaN    NaN   \n",
      "question_id                             0.103     -0.060        -0.098 -0.011   \n",
      "dataset                                   NaN        NaN           NaN    NaN   \n",
      "user_query                                NaN        NaN           NaN    NaN   \n",
      "sample_class                              NaN        NaN           NaN    NaN   \n",
      "split                                     NaN        NaN           NaN    NaN   \n",
      "qwen                                    0.045      0.556         0.693  0.325   \n",
      "mistral                                 0.150      0.062         0.026 -0.003   \n",
      "gemma                                   0.095      0.847         0.575  0.132   \n",
      "y_true                                  0.041      0.591         0.650  0.194   \n",
      "llama                                   0.002      0.428         0.427  0.206   \n",
      "chatglm                                 0.044      0.445         0.498 -0.085   \n",
      "bge_retrieved_top1_id                   0.107     -0.039        -0.079 -0.013   \n",
      "bge_retrieved_top1_score                0.001      0.004         0.003  0.002   \n",
      "bge_keyword_retrieved_top1_id           0.107     -0.039        -0.079 -0.013   \n",
      "bge_keyword_retrieved_top1_score        0.001      0.004         0.003  0.002   \n",
      "bge_top1_doc_id                           NaN        NaN           NaN    NaN   \n",
      "bge_top1_doc_score                        NaN        NaN           NaN    NaN   \n",
      "qwen_aug                                0.080      0.461         0.488  0.236   \n",
      "llama_aug                               0.109      0.479         0.415  0.137   \n",
      "mistral_aug                             1.000      0.161         0.059  0.002   \n",
      "gemma_aug                               0.161      1.000         0.557  0.129   \n",
      "neural_model                            0.059      0.557         1.000  0.215   \n",
      "ral2m                                   0.002      0.129         0.215  1.000   \n",
      "chatglm_aug                             0.095      0.472         0.494 -0.047   \n",
      "gpt5                                    0.013      0.126         0.006 -0.523   \n",
      "bge_intent_retrieved_top1               0.107     -0.039        -0.079 -0.013   \n",
      "bge_intent_retrieved_top1_score         0.001      0.004         0.003  0.002   \n",
      "\n",
      "                                  chatglm_aug   gpt5  \\\n",
      "query_id                                  NaN    NaN   \n",
      "question_id                            -0.078 -0.062   \n",
      "dataset                                   NaN    NaN   \n",
      "user_query                                NaN    NaN   \n",
      "sample_class                              NaN    NaN   \n",
      "split                                     NaN    NaN   \n",
      "qwen                                    0.356 -0.105   \n",
      "mistral                                 0.085  0.017   \n",
      "gemma                                   0.477  0.135   \n",
      "y_true                                  0.464  0.001   \n",
      "llama                                   0.325 -0.021   \n",
      "chatglm                                 0.904  0.206   \n",
      "bge_retrieved_top1_id                  -0.045 -0.054   \n",
      "bge_retrieved_top1_score                0.002  0.007   \n",
      "bge_keyword_retrieved_top1_id          -0.045 -0.054   \n",
      "bge_keyword_retrieved_top1_score        0.002  0.007   \n",
      "bge_top1_doc_id                           NaN    NaN   \n",
      "bge_top1_doc_score                        NaN    NaN   \n",
      "qwen_aug                                0.267 -0.105   \n",
      "llama_aug                               0.382  0.038   \n",
      "mistral_aug                             0.095  0.013   \n",
      "gemma_aug                               0.472  0.126   \n",
      "neural_model                            0.494  0.006   \n",
      "ral2m                                  -0.047 -0.523   \n",
      "chatglm_aug                             1.000  0.188   \n",
      "gpt5                                    0.188  1.000   \n",
      "bge_intent_retrieved_top1              -0.045 -0.054   \n",
      "bge_intent_retrieved_top1_score         0.002  0.007   \n",
      "\n",
      "                                  bge_intent_retrieved_top1  \\\n",
      "query_id                                                NaN   \n",
      "question_id                                           0.952   \n",
      "dataset                                                 NaN   \n",
      "user_query                                              NaN   \n",
      "sample_class                                            NaN   \n",
      "split                                                   NaN   \n",
      "qwen                                                 -0.141   \n",
      "mistral                                               0.022   \n",
      "gemma                                                -0.060   \n",
      "y_true                                               -0.079   \n",
      "llama                                                -0.051   \n",
      "chatglm                                              -0.067   \n",
      "bge_retrieved_top1_id                                 1.000   \n",
      "bge_retrieved_top1_score                             -0.000   \n",
      "bge_keyword_retrieved_top1_id                         1.000   \n",
      "bge_keyword_retrieved_top1_score                     -0.000   \n",
      "bge_top1_doc_id                                         NaN   \n",
      "bge_top1_doc_score                                      NaN   \n",
      "qwen_aug                                             -0.096   \n",
      "llama_aug                                            -0.073   \n",
      "mistral_aug                                           0.107   \n",
      "gemma_aug                                            -0.039   \n",
      "neural_model                                         -0.079   \n",
      "ral2m                                                -0.013   \n",
      "chatglm_aug                                          -0.045   \n",
      "gpt5                                                 -0.054   \n",
      "bge_intent_retrieved_top1                             1.000   \n",
      "bge_intent_retrieved_top1_score                      -0.000   \n",
      "\n",
      "                                  bge_intent_retrieved_top1_score  \n",
      "query_id                                                      NaN  \n",
      "question_id                                                -0.000  \n",
      "dataset                                                       NaN  \n",
      "user_query                                                    NaN  \n",
      "sample_class                                                  NaN  \n",
      "split                                                         NaN  \n",
      "qwen                                                        0.005  \n",
      "mistral                                                     0.000  \n",
      "gemma                                                       0.003  \n",
      "y_true                                                     -0.003  \n",
      "llama                                                       0.005  \n",
      "chatglm                                                     0.002  \n",
      "bge_retrieved_top1_id                                      -0.000  \n",
      "bge_retrieved_top1_score                                    1.000  \n",
      "bge_keyword_retrieved_top1_id                              -0.000  \n",
      "bge_keyword_retrieved_top1_score                            1.000  \n",
      "bge_top1_doc_id                                               NaN  \n",
      "bge_top1_doc_score                                            NaN  \n",
      "qwen_aug                                                    0.007  \n",
      "llama_aug                                                   0.004  \n",
      "mistral_aug                                                 0.001  \n",
      "gemma_aug                                                   0.004  \n",
      "neural_model                                                0.003  \n",
      "ral2m                                                       0.002  \n",
      "chatglm_aug                                                 0.002  \n",
      "gpt5                                                        0.007  \n",
      "bge_intent_retrieved_top1                                  -0.000  \n",
      "bge_intent_retrieved_top1_score                             1.000  \n",
      "\n",
      "[28 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "df_subset = df\n",
    "df_subset = df_subset.apply(pd.to_numeric, errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# --- 2. Models ---\n",
    "models = ['qwen', 'llama', 'mistral', 'gemma', 'chatglm']\n",
    "\n",
    "# --- 3. Cohen's Kappa Matrix ---\n",
    "kappa_matrix = pd.DataFrame(index=models, columns=models, dtype=float)\n",
    "for c1, c2 in combinations(models, 2):\n",
    "    k = cohen_kappa_score(df_subset[c1], df_subset[c2])\n",
    "    kappa_matrix.loc[c1, c2] = k\n",
    "    kappa_matrix.loc[c2, c1] = k\n",
    "kappa_matrix.fillna(1.0, inplace=True)\n",
    "\n",
    "# --- 4. Pearson Correlation Matrix ---\n",
    "pearson_matrix = df_subset.corr(method='pearson')\n",
    "\n",
    "# --- 5. Print Matrices ---\n",
    "print(\"\\n--- Cohen's Kappa Matrix ---\")\n",
    "print(kappa_matrix.round(3))\n",
    "\n",
    "print(\"\\n--- Pearson Correlation Matrix ---\")\n",
    "print(pearson_matrix.round(3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
